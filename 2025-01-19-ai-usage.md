---
layout: post
title: Grappling with AI Usage
date: 2025-01-19 07:22:05-0500
categories:
tags: [artificial-intelligence, harm, rant]
summary: Giving some thought to why people use language models
thumbnail: /blog/assets/125905185-human-ai.png
offset: -25%
description: Looking at the common threads behind treating AI systems as reliable workers.
spell: Doot Akiho JSkourr
proofed: true
---

* Ignore for ToC
{:toc}

Over the years, I have ended up talking about modern big-corporate AI---artificial neural networks trained as so-called large language models and image diffusion models that generally get strapped to chatbots, but also come in other forms---than I would've ever expected.  Since [*GitHub Copilot and Other Programming Doom*]({% post_url 2021-07-18-copilot %}), I've made no secret of my belief that it probably won't come to much.  While optical character recognition (OCR), classifying images, and so forth all make the underlying neural network technology respectable and important, shoveling all data into them in hopes that it will spontaneously learn to do any of our jobs seems highly improbable for a wide variety of reasons[^1].  For the purposes of this post, when I talk about AI, I mean the generative models strapped to chatbots, not the actual useful technologies that blend into the background.

[^1]:  If I had to pick, I'd call my favorite reason that, *if* you could train a neural network to perform actual intelligence, then you could also implement that intelligence in code.  And if you can implement intelligence in code, then you can follow the same algorithm *on paper*.  Second place goes to the fact that the companies tell us that, to become profitable, they "only" need a breakthrough in fusion power generation to get their costs under control, an exception to copyright law so that they can ingest whatever they please, and general intelligence to "emerge" for them to provide a product that people will actually find valuable, for which see the previous item.

Personally, I believe that using AI for anything more than personal entertainment shows a complete disregard for your work[^2], because by definition, the software can't *care* about what you need, so it almost literally "phones in" the text and visual art that it gives you.  The infamous errors manifest as a symptom of this, and don't matter nearly as much.  Even if we could make these systems technically perfect, and even if we could get train the networks only on works where people explicitly opted into that, the work still fits the purpose *less* than almost anything else.

[^2]:  And yes, I know that I have used a couple of AI-generated images on the blog.  I regret that in retrospect, and will eventually get around to replacing them, even though I deliberately used them in the context of lifeless, derivative work, to generate small elements to use in a more complex image, or as a literal placeholder.  In other words, it serves as something of a signal that I didn't care about that aspect.

![A human hand and a robotic hand reaching from opposite sides of the image to approach near the center](/blog/assets/125905185-human-ai.png "Doot.")

And yet, people seem to adopt them anyway.  I'd like to consider why, at least in a handful of categories, because I think that we can find a common thread other than assuming that the companies have conned them.

## Literal Faith

I can't help remembering a study from 2023, [*Thinking about God makes people more likely to trust AI*](https://www.futurity.org/god-trust-artificial-intelligence-2962972/).

 > People trust humans more than artificial intelligence, but when they think about God they are more likely to embrace AI recommendations over those from their peers.

And this study cites others suggesting that, "in general, religious people prefer AI recommendations more than non-religious people do."

In a frustrating turn, the study wanted to "enhance consumers’ openness to AI-based recommendations," but that feels like it underscores the importance of the results.  When culture has primed you to trust in mysterious powers, maybe the chatbot that tells you to put glue on pizza or whatever has a valid point that you don't understand.

## Except When I Know Better

We've all seen some version of the following imaginary post on social media, or maybe even in more reputable outlets, right?

 > AI Chatbots seem really useful, and I use them all the time, but keep in mind that it does a terrible job with the specific tasks that I do for a living.  Somehow, it gets all those questions wrong.

It amuses me, among many other people online, that you can find at least one version of this post for almost any field of expertise that you care to consider.  I didn't make it generic for the purposes of sounding coy, but because almost everybody believes that AI works well except for the thing that they know about.

If we aggregated all the things that AI does poorly, based on experts pointing out a single flaw, then we might need to start questioning what it *does* do well.  And yet, somehow, those experts never seem to notice that they've praised its responses for questions that they don't necessarily have a complete understanding of.

Alternatively, to make a blunter statement, they see value in cases where they won't or can't quickly check the results.  Where they can, they see through the system.

## Anecdotal

In my own life, I know a few people who use these chatbots regularly.  While some of them have made a careful, studied ignorance a core part of their unfortunate personalities, most of them frequently panic that they’ll "say the wrong thing" or otherwise have severe [impostor syndrome]({% post_url 2020-08-23-imposter %}) of one variety or another, and so assume that a multi-billion-dollar machine that exists to waste [electricity <i class="fas fa-copyright"></i>](https://www.tomsguide.com/ai/chatgpt-energy-emergency-heres-how-much-electricity-openai-and-others-are-sucking-up-per-week) and [water <i class="fas fa-copyright"></i>](https://fortune.com/2024/09/23/ai-water-usage-droughts-chatgpt-microsoft/) must do better than they would, even though it demonstrably does far worse.  I especially note this discrepancy in personal correspondence, where everything that they say sounds like the end of a conversation, no matter the content, because the prompt presumably only said to write a message with certain contents.

In many ways, it reminds me a lot of people who had no faith in their writing ability back in the 1990s.  Often, they'd have spell-checkers autocorrect everything for them without reviewing the results, leading to them offending pretty much everybody who they addressed in e-mail, by mangling their names into the nearest dictionary word.

I've seen a surprising number of people dismiss the value of helping me (or trying to help, at least) with something outside their expertise, because "oh, I don't do that well, so I had ChatGPT do most of it."  Coincidentally, I needed to redo a lot of that work, because these chatbots rarely adhere to all requirements, but that misses the point.

We can see this sentiment echoed in advertising, by the way.  You should use chatbots, they say, to turn your weak list of bullet points into a full e-mail, so that you look more professional.  And then you should use those same chatbots to turn the lengthy e-mails that you receive into a list of bullet points, so that you look smart for having processed the text quickly.  Mind you, everybody could've sent and read the original bullet points, but never mind that.  Also, you should ask the chatbot to write a thank-you note to your friend for watering your plants when you went on vacation, even though the request prompt literally contains every word that will go into that note.  Children should use chatbots to send fan mail to celebrities, because apparently every child aspires to technical correctness over personal connection.  Advertising doesn't *always* prey on audience insecurities, but it happens often enough that we should always consider the possibility.

Even on my own, by the way, when I have tried to use language models for productive work---as opposed to only prodding it to see if I can coerce it into doing anything useful---I've done so because I ran out of ideas and couldn't find anything that I needed on a web search.  It never actually *helped*, but my model use case for the technology looks an awful lot like despair, or at least a crisis of faith in myself...

## Students

This group most worries observers, I think, in the same way that computers and calculators worried people in my earlier days.  You can't go more than a couple of weeks without some media outlet wringing its hands over students using AI to complete their assignments, often furrowing their brows because the chatbots perform well enough on those projects.  Usually, they see this as a reason to condemn the students or the tools, rather than assignments so bland and formulaic---lifeless---that software can do it well, even when it can't do much else with any credibility.

As I said, we've actually heard versions of this complaint before.  During my time at school, we had arguments over the legitimacy of using calculators.  For at least part of the time when I [taught]({% post_url 2020-01-19-teaching %}), I had students with laptops out, connected to the school's network, sometimes needing them for their jobs.  I mentioned spell-checkers before, which came with their own crisis, as has the ability to type and print---rather than hand-write---assignments.

To some, though, this feels different, in that it "solves" a long-form problem, such as writing a [five-paragraph essay](https://en.wikipedia.org/wiki/Five-paragraph_essay).  But I'll tell you, not many students think they write those essays well, because they (at least) *feel like* rote work devoid of meaning.  You describe the topic and your alleged position, write about three and only three arguments for the position that (notably) fit into a single paragraph, and then declare victory, without introducing any contradictory evidence.

## The Falling Sky

Recently, a podcast actually brought a lot of this into focus.  Let's check out **On the Media**'s recent [*How AI and Algorithms Are Transforming Music* <i class="fas fa-copyright"></i>](https://www.wnycstudios.org/podcasts/otm/articles/how-ai-and-algorithms-are-transforming-music?tab=transcript#:~:text=know%20this%20sounds%20dramatic), the third segment with composer Mark Henry Phillips.  He talks about how this new technology---an AI that writes songs---will put him out of work, because the AI does a better job than him for the work that he often does.

What kind of work does he do?  Phillips, like many composers who you haven't heard of, writes...for lack of a better term, [*slop*](https://en.wikipedia.org/wiki/AI_slop).  I don't mean that he supplies bad music---he seems to write some fairly good music, in fact---but rather that he supplies the sort of "placeholder" music that you'd look for or commission cheaply when you want a general vibe or that background that evokes somebody else's work without paying them.  This might mischaracterize his description slightly, but hearing him talk about it, I get the sense that you hire him when you want background music that "sounds Japanese" or a jingle that "could work" as a riff by an A-list musician.  You wouldn't hire John Williams or Andy Akiho for that, because they have their own styles and probably cost more than the project could pay.  Instead, you get a cheap copycat.  In the Free Culture space, you'd look for someone like [Kevin MacLeod](https://incompetech.com/music/royalty-free/music.html) or something that he has written and published, never innovative, but "good-enough" slop that probably sounds like something else.

Anyway, by those standards, if any of us could go to an AI-powered website and say "write me a commercial jingle about car insurance in the style of Bob Dylan" and get something barely credible, then yes, people like Phillips will have a much more difficult time selling their work, because the customers primarily care about paying the bare minimum for that work.  They go to Phillips, because they don't want to pay to commission a "real" songwriter, even though they want that style, and so they'll go to AI, because they don't really want to pay to commission Phillips, either.  They *prefer* lifeless and not really fit for purpose, because the music only needs to get the product in people's memories.  As long as it sounds like it has a high production value, that works far better than bespoke music recorded by a local band.

You'll notice, though that Phillips doesn't frame the problem that way.  Instead, he frames the problem as that the AI produces "really good" music.

However, hints of why he sees the value of AI-generated music occasionally poke through.  He talks in passing about the "many musicians out there who could have produced something way better than me or the AI."  He dismisses the songwriting process as one where he'll "mess around and discover the song" based on a vague goal, and he tries to contrast that with nearly every other art; insisting later that writers *know* what result they want.  And he goes out of his way to tell us that he won't turn in "amazing" work.  More importantly, near the end of his piece, he talks about using AI for his own work "like having the best music writing partner ever," wanting to use it to finish and fix his personal projects.

In other words, AI might replace Phillips, because he doesn't see value in his work, and works in a space where they don't value his work, either.  He produces "slop," because the clients want slop, and AI can produce that slop much cheaper.

## Billionaires

I want to throw in one more category, here, the current top of the AI food-chain, billionaire tech CEOs.

Meta's Mark Zuckerberg recently announced---not at all related to his suppression of internal criticism of his recent changes, I assume---that he expects to replace most of his developers with AI, this year.  Many other leaders have said similar things with various time-frames attached.  I don't take it seriously, for the same reason that I haven't taken it seriously that fast food restaurants have insisted that they'd replace their kitchens with cooking robots "soon" for most of my life.  They use it as a threat to devalue their labor force and---if everything breaks well for them---drive down salaries.

Meanwhile, over at AI companies, they keep insisting that [artificial general intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence) will *probably* "emerge" [Real Soon Now](https://en.wiktionary.org/wiki/real_soon_now){% emoji trademark %}.  And that exposes the other reason that I laugh at the pronouncements of replacing knowledge workers:  The CEOs making these claims haven't the foggiest idea about what the jobs actually entail.

Moreover, they see employees the same way that we see the AI chatbots, uninterested in the work, doing strange things to produce results that may or may not solve the problem, and probably using up more resources to reach the solution than absolutely required.  But the AI chatbot won't try to unionize the shop, making it better, in their eyes...

## Consensus

I can't help seeing the commonality, here, that almost everybody that I see heavily using language models seems to suffer from a serious lack of confidence.  Either they don't understand their work, don't trust themselves to turn in good work, or they don't see the work as having enough value to put in the effort.  I can't find anybody saying that they already do good work and AI makes the work or them better.  I can't find anybody saying that AI makes for a better fit.  They say that AI does a good job at things that they don't know about, maybe, or that it handles tasks that they don't value.

And I'll tell you, that seems like a *much* worse problem than anything else related to AI.  We shouldn't have this many people who believe that a machine that doesn't care about anything can out-perform them.  We really shouldn't.  I wish that we had some way of fixing that, other than individually teaching each person that they should enjoy making bad art.

I also wish that I didn't have to end this on such a down-note, but I wanted to call out this confidence issue so that we can maybe start treating the cause of the problem, rather than its symptoms...

* * *

**Credits**:  The header image is [HUMAN-AI](https://commons.wikimedia.org/w/index.php?curid=125905185) by [JSkourr](https://commons.wikimedia.org/w/index.php?title=User:JSkourr&action=edit&redlink=1), made available under the terms of the [Creative Commons Attribution Share-Alike 4.0 International](https://creativecommons.org/licenses/by-sa/4.0/deed.en) license.
