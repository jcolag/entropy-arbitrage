---
layout: post
title: Using (or Not) AI
date: 2026-01-18 06:54:12-0500
categories:
tags: [artificial-intelligence, programming, rant]
labels: [career, personal]
summary: Answering the ubiquitous question of how one best uses so-called AI in our work
thumbnail: /blog/assets/Artificial-Intelligence-No-hands-fractal.png
offset: -20%
description: AI has more to it than generating slop, and we should distinguish between those uses.
spell: PRjuOD borderColor fillColor ejFjIf thumpety VRPQxf WT CWl sealioning dpGwHI YDoEIs Angermann
proofed: true
---

<script src="/blog/assets/chart.js"></script>
{% include mathjax.html %}


* Ignore for ToC
{:toc}

At the risk of wasting everybody's time with my personal life for a moment, to give this post some context, I spent a lot of 2025 looking for a job[^rYzJl3], meaning that I have spent a lot of time filling out the most insipid job applications that I have seen in decades.

[^rYzJl3]:  That search continues into 2026, to bore people even more.  If anybody has leads, I hope that I have provided plenty of evidence from six years of blogging that I happily do a little of everything and know how to work through problems, as well as a sense of what I enjoy doing, please get in contact...

![A painting of a jester-like hand-puppet, but it wears a copy of itself on its hands, each of which has copies on its, and so on](/blog/assets/Artificial-Intelligence-No-hands-fractal.png "Painted decades ago, yet fractal slop has never seen so relevant...")

In that mess, I have found one recurring question that consistently annoys me, both in its ominous tidings for the nature of the job, and because it feels like it exposes whoever wrote or chose the question as a complete poser.

> How do you use AI in your work?

You can see my former problem right away, I imagine.  They seem to want to receive something such as this.

> I vibe-vibed my way to vibe-coding my vibes, you vibe?  I actually only exist in my work *as* vibes while a fleet of agents makes me completely disposable, since I only ever really programmed by looking things up on Stack Overflow and letting the compiler remind me to change the variable names.

They seem to want to select for people who will slop out dodgy code---we'll get to that---and who they can freely throw under the bus whenever that Write Never, Ship Often policy leads to consequences for the company, because they don't *really* do anything anyway, a higher-stakes return to the days when we measured productivity in lines of code written per day.  The question signals that they don't want a professional who'll avert those consequences by thinking things through.  I could have misjudged them, sure, but given that the vendors pitch their chatbots as substituting for expertise *and* we have over thirty years of seeing how companies have used offshore labor, I don't think that I have.

For the other problem...?

I needed enough nuance in my answer to answer both problems, so I have written a full version to copy and paste.  Adding explanations and plenty of asides that I assume that readers here have come to expect, and that answer grew to become this post, so buckle in.

## Why Do You Ask

Do you remember where I mentioned seeing whoever used this question as only posing as professional?  We'll start there, because "AI" as a 2020s marketing term has narrowed the conversation around artificial intelligence to an absurd degree, and I can't help fighting back against that.

Therefore, I point out that, in my work, I have spent decades using and setting up adaptive Bayesian filters, broom-balancing[^_KPF5o] optimizations, contextual auto-complete, and optical character recognition to get users and customers where they need to go, using some of the best artificial intelligence available.  More recently, I have started deploying embedding models for semantic searches.  It all works with trade-offs, somewhat slower processing, but much less investment in developing specific algorithms to try adapting to every situation.

[^_KPF5o]:  It looks like people don't commonly use the term anymore, so to explain:  Imagine literally trying to balance a broom on the palm of your hand, bristles up.  You can probably do this fairly well with not much more than a couple of minutes' practice, because our brains have a fairly good intuition for finding the center of mass.  It takes *much* more effort to program a computer to do this, because you have a chaotic system where each wrong move compounds the problems, and overcompensating redirects the problem in the opposite direction.  However, you *can* train a neural network to do this without much trouble, so little trouble that AI classes of past decades sometimes assigned doing so as homework.  Why care, given how infrequently circumstances demand that we balance brooms on our robot hands?  The process generalizes quickly to pushing multipart objects through complex spaces, such as---probably the most economically valuable use---backing up a tractor-trailer smoothly at a target like a loading dock.  I'd bet that ever "parking assist" feature in a motor vehicle uses exactly the same undergraduate homework assignment plus (the expensive part) some robotics.

I should note, here, that I only listed the uses of AI systems that I have personally used on a job, in that last paragraph.  More broadly, you also have computer vision that can handle tasks ranging from flagging medical scans as maybe worth a second look by doctors to alerting researchers to the likely presence of a specific kind of animal or a celestial object.  You have voice recognition and transcription, similar for handwriting recognition.  Some great systems exist that can take a handful of photographs of an area and put together a 3D image.  Personally, in many cases, I probably couldn't manage without machine translation.

Less valuable, but still interesting, you have the world of game-playing, like chess engines planning out possible moves.  Decision trees can index information so that "normal" users can find it without needing to know what they want in advance.

Sometimes, these systems compound.  For example, for Arab American Heritage Month (April) and Asian American and Pacific Islander Heritage (May), I can't always find pre-packaged quotes from public domain sources for my Mastodon feed.  When I run out, I often need to turn to scans of fiction and poetry by writers from those groups.  The images go through a multi-language OCR application to convert the image to text in the author's native language, then sending that text through machine translation so that I can read it and find the interesting parts.

### Catty GPT

Does this opening seem petty?  Maybe a bit.  I do know what they *mean* to ask, after all, and stubbornly refuse to give it to them to make a point.  But by the same token, asking this question and assuming that only ChatGPT/Claude/whatever fits in the answer *should* feel similar[^VP2Nuz] to if somebody asked what programming languages you have used but secretly meant that they only wanted to know if you used [Pony](https://www.ponylang.io/), something that nobody who understands the terminology would rationally guess.  If the definition of artificial intelligence becomes an almost literal shibboleth, because some venture capitalists want you to think that nothing else in computing matters, then that tribalism will destroy the industry faster than the AI bubble popping will.  At least in my mind, I get paid to keep my employers from making those sorts of mistakes---if not, then why waste money on people with experience?---and that may as well start with the application.

[^VP2Nuz]:  While more niche, I should probably use the example, here, of *real-time processing* instead of programming languages.  If you have done much operating systems work, you know that people rarely use the term correctly.  It actually refers to tasks that have a strict deadline, which the system must either complete or abort before the deadline expires.  People who'd like to look smart, though, want it to mean "updated results show up on my screen quickly," though, causing problems in discussing the idea.  A pacemaker uses real-time programming, because it needs to either initiate the next beat on schedule or provide as much time as possible for the back-up plan, since "we'll pump blood after we finish processing the backlog" kills people.  Your gambling app doesn't do any such thing.

Plus, I'd call this significantly less petty than the application, early in my career, that sprung *do you have familiarity with technologies?* on the final page.  There, I wrote a response mentioning wheels and inclined planes before trying to guess what Big Tech stack that they might have meant.  By contrast, this barely shows up on the pettiness graph...

When I say that asking a question like this makes the person look like a poser, I mean this exact situation.  They give the impression of having no sense of history or even of ever actually getting things done, instead only wanting to chase the fad so that the other kids at the bus stop won't make fun of them.  I don't mind working with such people, but I feel a duty to educate them.  And if they don't like it, better that they see it long before we meet than starting to resent it six months into the professional relationship.  Also, I see a tiny chance that some really sharp hiring manager would ask the *how you use AI* question to screen out the posers applying, so in some ways, that first part of the answer targets them, too.

Really, though, this space has always had so much exciting---both from a technical interest and economic value standpoint---work going on in it.  I can't help sharing that, and I can't help resenting that those venture capitalists would like to erase it all and pretend that we can't do better than Magnetic Poetry As a Service.  Generating plausible-looking text doesn't have much technically interesting about it, and the economic value...let's get to that.

## This Can Only End Well

Moving on, I follow that "we need to talk about real AI" bit with the answer to the question that they *believed* that they asked.  If they meant to ask when I have set up a chatbot to impress people or used one to do my work for me, I can assure them that I have.  And that it failed to solve the user's actual problem in entirely predictable ways.  It turns out that replacing a problem such as "I can't find the product that I want" with "the chatbot wrote me a mostly incorrect five-paragraph essay about the product that I asked for, rather than find it for me" hasn't improved the situation.

{% cw For those readers who care about such things, I didn't bother to note the licenses of material at the other ends of the links.  The various authors have made some available under Free licenses, but definitely not all.  Also, long-time readers may have seen many of these links before, not that they have become any less relevant. %}

Unlike the classical systems, generative AI has quickly become the most efficient way to make white collar work worse by every metric; I'll focus on developing software, since I know that well and it better fits the job application framing of the post, but it applies pretty much everywhere.  Not only do we have plenty of anecdotal evidence and increasing sentiment turning against the tools, we have solid research telling us that Large Language Models [slow work down](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/) by about twenty-five percent even while making many developers believe that it *increased* their speed by a similar amount, while writing [buggier code](https://www.coderabbit.ai/blog/state-of-ai-vs-human-code-generation-report) especially including [security vulnerabilities](https://www.securitymagazine.com/articles/101801-ai-introduces-security-vulnerabilities-within-code-in-45-of-cases).  And it makes the human developers [less capable](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/01/lee_2025_ai_critical_thinking_survey.pdf) of dealing with those problems, "leaving them atrophied and unprepared," according to Microsoft researchers, with independent confirmation that [LLM users consistently underperform at neural, linguistic, and behavioral levels](https://arxiv.org/abs/2506.08872), and that the [risks of AI in education outweigh the benefits](https://www.brookings.edu/articles/a-new-direction-for-students-in-an-ai-world-prosper-prepare-protect/), for good measure.

We should talk about those anecdotes, though, too.  For almost any information-based activity that you care to name, professional or hobby, rigorous or creative, you can find people who want you to know two things.  First, they love to use these AI chatbots to do a lot of their work for them, because it does such a great job at everything.  But also, they'd like to warn you against using the tools for the area where they have expertise, because it mysteriously does a terrible job at that stuff.  Weird, right?  How could the bot ever know what the user had experience with, so that it can choose to do a bad job on those topics?  I actually wrote a post about a year ago [connecting AI usage with a lack of confidence]({% post_url 2025-01-19-ai-usage %}) by looking at different categories of the work.

Oh, and the [models themselves have degraded](https://spectrum.ieee.org/ai-coding-degrades), expected to continue that trend, so you get all that overhead with less benefit---if you see any benefit in there at all---over time.

No, really.  The data tells us that, not my *old guy refusing to keep up with the times* vibe.  AI coding assistants have now had years to prove that they can improve the development process, that they can finally resolve the [software crisis](https://en.wikipedia.org/wiki/Software_crisis), and have instead only found ways to make the crisis worse while also somehow making *developers* worse at everything.

### Numbers

To quantify that data for readers here, the above studies mean that, for every dollar that a company would spend on a professional programmer working as we have for eighty or so years, that labor now costs a dollar twenty-five to start, because the AI takes more time and effort to get to a passable solution.  Many developers will *estimate* the same work as costing seventy-five cents, meanwhile, so a hypothetical bidding process means that projects will already have a cost overrun of two-third of the bid, one-and-a-quarter divided by three-quarters, or five-thirds of the estimate; for non-bidding work, you see the same thing, but the estimates show up elsewhere.  On top of that, the Code Rabbit report shows an additional four percent incidents[^II79EO] per change, annually, meaning that the dollar's worth of work probably costs something more like a dollar-thirty---we resolve incidents by writing more code---giving us more than a seventy-three percent overrun from the estimated work.

[^II79EO]:  I can't help noticing that they refer to an increase from twenty to almost twenty-four percent incidents, or a twenty percent increase *on* the ordinary twenty percent.  That strikes me as interesting, given that the speed increases/decreases sit at roughly the same magnitude.  As in, coding assistants make you slower, make you *think* that they make you faster, and make code worse by roughly a fifth to a quarter more, consistency which smells suspicious.

Oh, and regardless of the number of incidents registered in the field, an absurd seventy-five percent of the changes have logic errors.  Someone will need to fix those at some point, but maybe we could plug our ears and sing when people talk about those, only fixing them when something *really* goes wrong that costs us money instead of only harming the customers' wallets or data.  And, oh, let's not forget that the Security Magazine reporting tells us that forty-five percent of changes have actual security vulnerabilities.  If we don't catch and fix those immediately, security issues now move the cost from labor to litigation, currently [costing companies three million dollars](https://www.computerweekly.com/news/366622911/Data-breach-class-action-costs-mount-up) on average per incident, so you probably don't want to put off fixing those...ideally in a way that doesn't create new security problems in almost half *its* changes, or you'll end up using [Zeno's paradox](https://en.wikipedia.org/wiki/Zeno%27s_paradoxes) as data security strategy.

Oh, and back to the labor cost, according to Microsoft---who wants to push language models into everything, by the way---and those independent researchers, you can expect declining performance from the employees that you pay that extra money to wrangle AI assistants.  In our quick model, that means that the "extra" four percent incidents that need resolving will take longer than the time-and-a-quarter that AI-assisted workers already take.  Oh, and remember that those incidents occur *per year*, meaning that they'll compound with new errors, exponentially increasing the cost over time.  That happens with any project, yes, but that four percent per year over time increases the numbers faster.  How much faster?

$$\begin{equation}E = 1.235^{t}-1.2^{t}\end{equation}$$

This doesn't *sound* like much of a difference, I admit, unless you work regularly with exponents, but it escalates quickly, the quantity of errors doubling a bit less than every two years.  At eight years, that extra four percent crosses the equivalent of an extra year's worth of errors.  Two years later, more than two years' worth.  Five years later, more than eight years.

{% chart PRjuOD|463|{
  type: 'line',
  data: {
    datasets: [
      {
        label: 'LLM Error Overhead',
        data: [
          0,
          0.0350000000000001,
          0.0852250000000003,
          0.155652875000001,
          0.252711300625001,
          0.384674456271877,
          0.562164153495768,
          0.798782169567274,
          1.11190730741558,
          1.52369911825825,
          2.06236072336894,
          2.76372626814464,
          3.67325487089943,
          4.84853328124975,
          6.3624148211702,
          8.3069537667373,
          10.7983336570311,
          13.983036972566,
          18.0455645484782,
          23.2180888822016,
          29.7925197673161,
          38.135577909992,
          48.7076179156681,
          62.0861231620437,
          78.9950201485562,
          100.341239535586,
        ],
        fill: true,
        borderColor: '#49281b',
        fillColor: '#49281b20',
        tension: 0.1,
      },
    ],
    labels: Array.from(Array(26).keys()),
  },
  options: {
    responsive: true,
  }
} %}

I didn't bother to put both pre- and post-LLM error rates on a chart, but you might also want to know that the latter gets near twice the former in the twenty-third year.

Granted, your company might not last twenty-five years, but if it does---and remember, we collectively spent more than [three hundred million dollars](https://en.wikipedia.org/wiki/Year_2000_problem#Cost) fixing code that nobody expected to last to the end of the century[^ejFjIf]---then you can look forward to a hundred years' worth of errors to fix on top of what you would normally need to deal with.  And based on the IEEE article, we can expect those error rates to increase, increasing the slope of that curve.

[^ejFjIf]:  Random thought, did scrambling to fix Y2K errors contribute to popping the dot-com bubble?  It only occurs to me now that Internet stocks peaked only months after the industry shelled out all that money, before dropping like a rock.  We often explain the bubble with interest rates, investment in companies with vaporware products, and companies splurging on marketing instead of manifesting those products, but now I wonder if the realization that code costs a ton of money to maintain might have tamped down excitement, too.

In other words, AI-enabled programming costs at least a third more in labor alone, *and* your team will blow its estimates by almost three-quarters, and fixing problems creates about three-quarters more problems, which means spending more than twice what you expect on labor, with that number increasing over time.  It also opens the company to lawsuits over data breaches, and if you use the same approach to resolving those data breaches, then each solution will likely create a new half-sized problem in place of the original.  On top of that, your staff becomes less capable, requiring even more time to complete tasks, using models that do worse work.

And this only looks at using the technology from the perspective of the team's budget.  The superficial analysis doesn't take into account alienating customers, opportunity costs due to those missed deadlines, damaged reputations, and so forth.  If you think that you can't alienate customers enough to drive them away, keep in mind that the [EU started abandoning US-run services](https://www.zdnet.com/article/europes-plan-to-ditch-us-tech-giants-is-built-on-open-source-and-its-gaining-steam/) for significantly less provocation than low quality delivered late.

### Doing the Wrong Thing

Taking this another step further than the answer that I give prospective employers, I would reiterate that the AI companies also do the part that nobody with any experience finds difficult.  Anybody can churn out mediocre code with plenty of errors and vulnerabilities.  When I started out, we used to joke---in contrast to "the old days," when relying on the compiler to catch your errors would take longer than reading through manually and running through the process on paper---about programming as "debugging into existence," writing any garbage and progressively resolving the compiler errors, then the logic errors, until the code worked.  You *shouldn't* work that way, burning time by not thinking things through, but you can.  When I say that anybody can do this work, I literally mean anybody.  You only need enough interest to keep at it until you finish the job.

Spewing out the first draft faster doesn't significantly impact the time to completion of the task, because you have all the difficult work remaining.

Not too long ago, in another post, I mentioned that we *could*---theoretically, I mean---use this generative AI to [find dangerous patterns and search for logic errors]({% post_url 2025-06-08-ai-wish-list %}) in human-written code, for example by classifying many changes by how quickly a developer needed to fix the fix, and using that to guess how well the current change will work.  But the companies have made it clear that they would rather exploit the [ELIZA effect](https://en.wikipedia.org/wiki/ELIZA_effect) and [psychic cons](https://softwarecrisis.dev/letters/llmentalist/) to convince people that [Frosty the Snowman will come back again someday]({% post_url 2024-02-25-ai-doom %}), thumpety-thump-thump and so forth, and so they need more venture capital to [cover their losses](https://techstartups.com/2025/10/31/openai-is-hemorrhaging-billions-microsoft-filing-reveals-openai-lost-11-5-billion-last-quarter-amid-ai-bubble-hype/), can't even [acknowledge copyright](https://www.theguardian.com/technology/2024/jan/08/ai-tools-chatgpt-copyrighted-material-openai) let alone care about it---more on that later---and need [fusion power to work](https://www.reuters.com/technology/openai-ceo-altman-says-davos-future-ai-depends-energy-breakthrough-2024-01-16/), so that they can [create their weirdo libertarian god](https://www.techdirt.com/2023/11/30/
elon-says-copyright-ai-lawsuits-dont-matter-because-digital-god-will-arrive-before-theyre-decided/).  And by the way, please [stop making fun of their useless products](https://www.windowscentral.com/microsoft/microsoft-ceo-satya-nadella-really-wants-you-to-stop-calling-ai-slop-in-2026), because it [makes them sad](https://www.inc.com/leila-sheridan/jensen-huang-has-had-it-with-your-ai-slander/91287603).  All this, despite the fact that the industry's [promises haven't changed in nearly a human lifetime](https://theconversation.com/weve-been-here-before-ai-promised-humanlike-machines-in-1958-222700).

### Explanation

I should note that all these problems---the extra time, the errors, only generating code instead of doing something more valuable, and so forth---have the same cause, which I have talked about before:  AI *does not care*.

It doesn't care what previous developers intended by their code, only what it currently does, in the most mechanical way.  An AI tool doesn't even "see" the code, only the semantic vector[^9qdq8l] of the explanation attached to it.

[^9qdq8l]:  Semantic vectors get back to the idea in the early section about using embedding models for semantic searches.  As you probably know, Large Language Models don't actually read.  They turn your input into a series of "tokens," numbers that represent syntactic elements, usually chunks of a word.  Then it takes those tokens, and generates an "embedding," the semantic vector, an array of numbers---1024, in cases that I've looked at---between zero and one, indicating a multidimensional "direction."  You train the embedding models so that the vectors for similar ideas point in similar directions.  Language models try to build answers that have a similar vector to prior known answers to questions that have vectors similar to your request. {% emoji woozy face %}

It doesn't care what some bug report meant, only how to respond plausibly to that same semantic vector representing it.  For a real-world example, early in my career[^VRPQxf], working on a desktop application, the team assigned me a request asking if we could add some control to the screen to change the font-size for the labels (for the other controls) on the screen.  And yes, we certainly could do that quickly, but the request made me suspicious, as something that nobody ever wants to do.  Following the request back to its source, I discovered that the user asked because...in testing the German translation, the longer labels would sometimes slip under---or sometimes *over*, depending on how the developer laid out that control---the input box.  And we have many better ways to fix that, such as reorganizing the screen or abbreviating the long translations.  No AI tool cares *why* you ask your question, only that you requested an answer.

[^VRPQxf]:  I knew that I'd get so much mileage out of this story (and a couple of others) that, at the time, I asked the company for permission to use it in examples, so that I could talk about it when teaching.  Normally, I avoid talking about real projects because I don't have permission to air other people's stories, but in this case, I actually do, provided that I don't provide specifics...

It doesn't care about selling its architecture to your colleagues and management.  You have an architecture from it, and if you like it, end of conversation.  If you don't, it'll praise your sharp and critical eye, then dump out an alternative.  Will it scale?  Will it play well with external systems?  If you didn't specify it in the request, in such a way that it makes it through the conversion to a semantic vector with all the rest of your context, then probably not, except by accident.

It doesn't care about maintainability.  If you have ever asked a chatbot to fix a specific problem with a block of code, you know this, because it almost invariably rewrites the entire thing, as if you never had the original.  Maintenance doesn't even register as a concept, because (in the short term) writing takes less effort than understanding and caring.

And it doesn't care about your company.  That lies on far too abstract a level to even make sense, let alone fit into its context.  You won't see Claude interrupting your task to point out that the design won't work well with the company that you have partnered with, or remind you that you probably shouldn't work on a risky task so close to a major release.  Cursor won't remind you about the five times that your colleagues pitched the same idea and needed to tear it out of the application.  ChatGPT won't tell you that your change would violate the platform user interface guidelines or the corporate branding guidelines.  Grok won't tell you that your feature already exists, meaning that you can save all the development and testing time by making it more discoverable to users.

The language model can *pretend* to care about these things on demand, sure.  In fact, if you have ever had one of the chatbots show their "reasoning," you'll quickly notice that the majority of its "internal narrative" directs it to saying things that resemble validating your emotions, rather than actually doing something.  And if you don't look at that skeptically, then you get people reading sincerity into [apologies for heinous acts](https://thehill.com/policy/technology/5669917-elon-musk-ai-chatbot-grok-apologizes/), when it only "knows" that it should respond to something with an angry semantic vector with appeasing language.  If you get angry, then it will toady harder.

No training can ever coerce these things into actually caring, because care doesn't come from language.  At some point in all our lives, we all wrestle with the problem that words do a terrible job of standing in for the things and people that we care about, let alone making it happen.  But language models only *have* language, so they mostly provide [Semantic Satiation](https://en.wikipedia.org/wiki/Semantic_satiation) As a Service while telling you how profusely they apologize, calculating that saying the words will convince you to continue the conversation.

### Roach Motel Computing

For all my concerns about *professionals* trusting these systems, I do want to make the point that every decade or two has had its programming system that makes itself approachable by abandoning anything that might encourage discipline or design.  It gets a new cohort of people excited about programming, and they write a lot of unmaintainable code while exploring it.

In my early years, we had [BASIC](https://en.wikipedia.org/wiki/BASIC), a language that, for decades, had almost no way of organizing the code in a way that a casual observer could make sense of any part of it without reading the entire thing.  Adding code sometimes required rewriting sections to literally make room.  Almost every aspect of the language made it difficult to produce anything that would survive for more than a few weeks.  Heck, most home computers couldn't even handle lowercase letters.  But you could walk into almost any store selling computers and type something like this into a display model.

```BASIC
10 PRINT "BOW DOWN TO GENIUS PROGRAMMER POOJA. ";
20 GOTO 10
RUN
```

And after doing that, everybody knows that you---the hypothetical Pooja, in this case---passed through, as your announcement scrolls around the screen, so kids (and other less-mature folks) learned enough to quickly throw together small programs to show off.  And most stores realized that this advertised the product for them, so they allowed it, which encouraged more people to learn.

After everybody agreed that we would never do that again---BASIC became a structured programming language and everything---as the world-wide web started to commercialize, [PHP](https://en.wikipedia.org/wiki/PHP) became the language to hate, teaching bad habits[^HOd9pY] by mixing control (the programming bits) with presentation (the HTML page), not having strong data types that can catch errors before you deploy the page, but having wildly inconsistent standard libraries.  But it not only made programming approachable to a new cohort, it filled the web programming niche that nobody else had really touched, and so powered the entire dot-com boom with *so much* technical debt that the bust probably saved those companies from needing to fix all that code.

[^HOd9pY]:  The language fixed most of its problems fairly quickly.  While PHP still carries that stigma from thirty years ago, to the consternation of the PHP community, the language makes a lot more sense today.

We have had versions of that since, and I feel like we could see AI coding assistants as the modern version of this.  They produce code that nobody can maintain, and teach you bad habits.  But if it brings you into programming, then you can unlearn all that when you realize that Copilot vibe-coded you into a quagmire, and you'll have a better chance of doing so with that interest sparked.  In the early days of my teaching, many of my best students came out of the dot-com bubble, learning the bare minimum to land a well-paying job, realizing that they enjoyed the work, and wanting a better foundation so that they could do more.

I don't care if it costs you twice as much to put your projects together, if it gets you excited about programming, though I worry that it making you a worse programmer over time will extinguish that excitement.  And more importantly, I wonder if and how we can catch and redirect "vibe-coders" before that happens.  How do we make sure that, after we lure you in with the bait, we can convince you to learn better practices before you escape the trap...?

## Copyrights and Fair Use

Now, let's go back to the problems with generative AI, and add in the potential *copyright* liability to go with the data breach liability that we talked about.

Training a language model almost certainly falls under Fair Use[^WT-CWl].  Generally speaking, courts allow a Fair Use defense when the use has some combination of (a) some non-commerce purpose, (b) a reason to target the source, (c) a less-than-substantial fraction of the source used, and (d) little effect on the market for the source.  The training itself checks all four boxes.  Although, also maybe not, because we now have evidence that the [models store the source material](https://arxiv.org/abs/2601.02671)[^7yj9Xo], which would eliminate that third factor from a Fair Use defense.

[^WT-CWl]:  On Mastodon a few months back, I tried to politely correct somebody who thought that Fair Use and the "human creation" requirement for copyright provided a loophole to launder copyrighted works into the public domain.  It quickly grew clear that this person preferred to sit in the "not even wrong" [sealioning](https://wondermark.com/c/1062/) space, getting angry because I wouldn't explain why different uses of the same material might constitute...different uses of that material.  Do I go through the reasoning here out of spite?  I'd say that I have more of a responsibility in blog posts to cover the entire story, than I would in a casual conversation with somebody making it clear that they won't listen, but you can make your own assessments.

[^7yj9Xo]:  Not that my anecdotal evidence should carry the same weight as a rigorous study, but I should mention, here, that I noted this early on in a post about [working with GitHub Copilot]({% post_url 2021-11-03-copilot2 %}#copilot-violate-a-license-for-me).  If a model can faithfully reproduce a text on demand, then no matter *how* they stored it, they definitely stored it.  If it works for a license, then I can't imagine it not working for code or prose, exactly the reason that I tested this.

Anyway, ignoring that glitch, selling access to the trained model *can* make a case for at least half of the factors.  But using the output of the trained model in a commercial product doesn't check *any* of the boxes, except maybe sometimes the third.  But even in the cases where somebody could theoretically make that third-element Fair Use defense, they don't actually **know** how much of other works that their output used, a serious obstacle to making that case.

Put another way, shingling together Fair Use results can still infringe copyright, even if you launder it through an AI, much like cherry-picking words and phrases from public domain works doesn't defend against the collection looking suspiciously like a famous novel.  You can't leap from [Fair Use in training](https://creativecommons.org/2023/02/17/fair-use-training-generative-ai/) to [copyright requiring a human author](https://en.wikipedia.org/wiki/Monkey_selfie_copyright_dispute) to imagining that you can have an AI write you a copy of a Taylor Swift song and assume that she can't sue you.  Or if you'd like a more concrete example, take [**Terminator the Second**](https://www.themoviedb.org/movie/1500419-terminator-the-second)[^dpGwHI].  They wrote, in effect, a stage-bound replica of **Terminator 2:  Judgment Day**, using only phrases found in Shakespeare's plays.  Pitching this as parody, it slides by on Fair Use, on almost all the possible criteria, most prominently that nobody would watch the play *as a replacement* for the film.  But if they tried to make it work as a serious production on its own, it absolutely violates copyright by reproducing every scene of the film, despite the writers taking no words (except for the proper nouns) from the movie script and having a chart that authenticates the Shakespearean source of every line of their play.

[^dpGwHI]:  I pledged to the Kickstarter campaign, way back when, and so still occasionally rewatch it on the "bootleg" DVD that they sent me as a reward.  If you ever have the opportunity to see it, do so.

In the same way, the fact that your vibe-coded app used a tool created by pushing Fair Use almost to the breaking point, and technically "writing" it through an entity that can't claim copyright doesn't take away any responsibility for *publishing* that app.

From that perspective, I can't imagine exposing a project to all that in hopes of nothing more than not looking resistant to new technologies.  And again, add this liability---smuggling in content protected by copyright---to that from poor security leading to data breaches and the increases in development costs.

## And Yet

Now, I have said this before, and I realize that this makes people skeptical of me, but I do believe that we have a useful technology in Large Language Models.  Semantic searches, and the embeddings that get us there allowing us to estimate similar intent between bits of text, seem highly valuable.  They do a decent (though unreliable) job of extracting key points from arbitrary text, such as finding proper nouns and what they refer to, something that you can't really program procedurally.  I can imagine the things doing a decent job of keeping to the voice of specific persona, such as a fictional character or a role.  If something seems necessary to have but not worth writing[^oB3x1], it can do that quickly.

[^oB3x1]:  In most cases, I suspect that you should *skip* that part of the project.  If it doesn't seem worth creating to you, then it doesn't sound at all worth experiencing to me.  Changing the context, imagine going to a restaurant.  The chef, there, doesn't like shopping for or cooking meat, so to save the effort, they replace any meat in their recipes with much cheaper wood shavings, so that you at least have *something* on your plate.  Amusingly, given the context of this post, I can think of one case where this feels appropriate:  Job applications traditionally have several questions that fit this category, such as the infamous *Why do you want to work with us?*, where generic slop answers the question far better (for the reader) than the brutally honest "my expenses require payment in money; you wish to exchange money for labor, and I can provide that labor in exchange for money."

More than a couple of lines at a time, though, the models don't generate code at all well.  Or rather, they generate code but not software.  They don't really generate anything of significance at all well.

We succeed in the software industry---probably any industry---by caring.  Most of us start out by caring about the technology or our own education, but as we gain more responsibilities, we start learning to care about our teams, about the needs that the code needs to fill, maybe about the health of the business, and more, and we start to see the technology as little more than a means to the end.  As Cory Doctorow put it recently, [Code Is a Liability](https://pluralistic.net/2026/01/06/1000x-liability/#graceful-failure-modes), except as a vector for caring.  And we know that you can't care on behalf of someone else, so we certainly can't sensibly care on behalf of some-*thing* else, either.

I like to joke, though, that "I love what the shop loves."  To finish off the narrative arc of my application answer, when it comes to technology choice on a job, I see my role as encouraging them to waste less and to do my best to work around obstacles to get working code out the door.  And if a company wants to pay me to babysit the system that encourages bad estimates, slows down work, and exposes the organization to multiple varieties of legal threat *while* wasting[^YDoEIs] absurd amounts of energy and water, I'd want to do my best to make sure that they understand the risks and mitigate them, while doing the stated job as well as I can.

[^YDoEIs]:  Don't forget that [DeepSeek](https://theconversation.com/deepseek-how-chinas-embrace-of-open-source-ai-caused-a-geopolitical-earthquake-249563) exposed that most mainstream models do waste far more energy than they need to use.  We could probably make them even less wasteful by "compiling" the simulated neural networks to direct and optimized matrix calculations.  And by the way, [Common Pile](https://arxiv.org/abs/2506.05209) similarly exposed that you can train a reasonably good language model without offending creators by screaming at them about Fair Use if they dare object, which goes nicely with the study showing that the models store all their training material.

And *that* answers how I use AI professionally...

* * *

**Credits**:  The header image is [Artificial Intelligence - No Hands Fractal](https://commons.wikimedia.org/wiki/File:Artificial_Intelligence_-_Nohandsfractal.jpg) by [Peter Angermann](https://commons.wikimedia.org/w/index.php?title=Peter_Angermann&action=edit&redlink=1), made available under the terms of the [Creative Commons Attribution Share-Alike 4.0 International](https://creativecommons.org/licenses/by-sa/4.0/deed.en) license.
