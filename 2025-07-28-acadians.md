---
layout: post
title: Developer Diary, Acadian Expulsion
date: 2025-07-28 07:22:05-0400
categories:
tags: [programming, project, dev-journal]
summary: Progress on assorted projects
thumbnail: /blog/assets/View-of-the-Plundering-and-Burning-of-the-City-of-Grymross-Thomas-Davies-1758.png
offset: -38%
description: This week's projects include the blog's newsletter, continued-but-maybe-wrapping computer woes, stenotype corner, the blog's code, and the Notoboto note-taking application.
spell: stenotype Typey YaCy SearX Notoboto Grymross forgejo Unidecode slugFromTitle Tcl ascii lmap expr xlit lindex fromstring áéíóúÁÉÍÓÚ ñçÑÇ üÜ öÖ zao ULID lmaadhr eodiyeyo aeiouaeiou ncnc uu oo
proofed: true
---

* Ignore for ToC
{:toc}

Today, at least for Eastern Canada and tiny parts of the United States (northern Maine), we memorialize the [Expulsion of the Acadians](https://en.wikipedia.org/wiki/Expulsion_of_the_Acadians), also known as the Grand Upheaval, an ethnic cleansing by British colonists, forcing out or killing about eighty percent of the native population, the survivors actively dodging capture.  If you know Longfellow's [*Evangeline*](https://en.wikipedia.org/wiki/Evangeline), then you know the broad strokes of the incident, except for descendants of the survivors fighting for recognition in court for two hundred, forty-three years.

![A View of the Plundering and Burning of the City of Grymross](/blog/assets/View-of-the-Plundering-and-Burning-of-the-City-of-Grymross-Thomas-Davies-1758.png "Apparently, the only known image at the time of any part of the atrocity.")

And with that downer, on we go to the projects.

## Entropy Arbitrage Newsletter

For those of you interested in such things, I'll have the next issue of the [**Entropy Arbitrage** newsletter](https://www.buymeacoffee.com/jcolag) ready to go on Saturday the second.

If you have signed up [on Mailchimp](https://entropy-arbitrage.mailchimpsites.com/), I still don't quite trust the company, but you'll get the e-mail on Saturday.  If you have subscribed on Buy Me a Coffee---at the link in the previous paragraph, click the *Follow* button to the upper-right of the page; no money will change hands---you'll get it on Tuesday morning, the fifth of August.  Since I never publish blog posts on Tuesdays, I find that a nicer match than Saturdays.

What will you find inside?  As always, you'll find links to all the articles that I found interesting in my RSS feed or bookmarked, plus some analysis of blog traffic.  For July, I wrote a piece on my thoughts about the new attempt to revive the Commodore 64, talked a bit about the most extreme nonsense that I've seen in this current job-hunt, discussed my media consumption slanted a bit towards celebrating Disability Pride Month, and provided some additional background information on **The Light's Edge** to tide people over until I can get my act together.  If you've become a member on Buy Me a Coffee, then you have already seen previews for some of that.

## Argh, Part 6

I think that I can see the oncoming train further in the tunnel, or something like that.

### Resettlement, Part 3

During the week, having verified that the cheap memory would work, and that I had the blank NVMe drive waiting in the old computer, I scrounged up an old USB drive to get an operating system installed.  At this point, then, the Framework laptop now has Pop! OS running on it.

Admittedly, System 76 left a bad taste in my mouth with seemingly shoddy hardware and not-so-great support, so going back to them for software feels like a stretch, but I found their desktop fairly comfortable.  And if anything goes wrong, I can always install Ubuntu or Debian over it without affecting much.

It will certainly take some time to get applications and data all set up...again.  But I do have the servers set up to limit my immediate necessities.  And the old laptop still hobbles along.  Between the two, I can afford to not rush.  That said, it appears that I accidentally found a short-cut in copying the contents of `.local/share` over, which triggered the system to demand upgrades of most of the applications that now had data, and so installed a bunch of things that I had actually forgotten about.

Upshot, though, I'll probably start using the Framework machine starting today.

### Mini-Server, Part 4

I don't have as much to report, here, beyond upgrading [Forgejo](https://forgejo.org/) to the new v12 release.  The instructions make the process sound utterly terrifying, insisting that you verify that it doesn't have any errors before making a backup.  In reality, though, you download the binary, replace the old one, and restart the service, and the backup only serves as a safety net.

In that process, though, I did have a rough time trying to confirm that I had no errors.  For no reason that I can find, Forgejo apparently can't find its own configuration, so if you follow the directions to call `forgejo doctor` (to scan for errors) or `forgejo dump` (to get the backup), it'll complain that it has no idea what you want.  In case anybody needs to know, I needed to add `--config /etc/forgejo/app.ini` to every command...

Actually, scratch the "nothing interesting to report" bit.  I installed a [YaCy](https://yacy.net/) node, so I now have access to the only distributed search engine that I know of, meaning (among other things) that I no longer need to go through the major search providers.  I have previously enjoyed and recommended the various branches of [SearX](https://searx.github.io/searx/), which scrapes through search results from the major engines and compiles them without the AI slop, ads, and other "results" designed more to keep you on the site looking at ads than getting your work done.  And I still mostly stand by that, though the major engines seem to have started cutting back their API quotas while making search results worse, which stifles the meta-search much of the time.  And with DuckDuckGo trying to hop on the AI bandwagon to nobody's discernible benefit, *their* reliance on outside search indices starts to make them look far less useful, too.

Anyway, YaCy doesn't do any of that.  By contrast...actually, rather than try to explain it, I'll cite [Wikipedia's article](https://en.wikipedia.org/wiki/YaCy) on YaCy, which seems fairly clear.

> Each YaCy-peer independently crawls through the Internet, analyzes and indexes found web pages, and stores indexing results in a common database (so-called index) which is shared with other YaCy-peers using principles of peer-to-peer.

In other words, I might not end up with as robust or consistent a search index, as peers jump on and off the network, and depending on how much time and space they dedicate to indexing, but I *do* now get searches based on an index independent of any major companies.  Oh, and YaCy behaves nicely behind NGINX, so I can redirect `/yacy` to the system's port 8090, and it works without any further configuration.

That said, I have two caveats that will probably prevent me from using this as my primary search engine:  First, using [Java](https://www.java.com/en/) on a low-power computer, it runs *slow*, and sometimes crashes mid-search.  Second, the aforementioned inconsistencies feel extremely jarring, where sometimes a search will only return a couple of obscure results, and other times it'll return dozens of varying quality soon afterward.

Oh, and speaking of caveats, you'll want to follow the [Raspberry Pi-specific instructions](https://yacy.net/installation/raspberry_pi/) for that system, not the general installation, even though...they looked the same to me.  The main instructions had Java throwing all sorts of weird errors that I had no interest in debugging, whereas the linked instructions work fine.

Note that, if you want to try YaCy out for yourself, it'll almost certainly take you less time to get it running locally than find a working instance available to the public[^1].  If you have a Java runtime installed, then you only need to download and unpack the right archive and run the start script for your system.  I set it up as a service and had the web server give it an easier-to-remember path, and the whole process (other than the false starts) only took me about ten minutes.

[^1]:  The project's "demo instance" hasn't worked in years, as far as I can tell, and they actually strongly suggest running your own instance, because you can't confirm that random sites have official code running.  I would also add that, having used such fly-by-night instances for non-sensitive searches, they don't usually last long enough to make them worth bookmarking.  And again, if you understand what "install Java" means, and don't put your mouth under the coffee urn's spigot, then it probably took you more time to read this post than get it to work.

Maybe the most interesting aspect of this, as I become more secure in this setup, I can presumably set up my own crawlers to index sites that I care about more, making the index "better"---assuming that you care about what I do---for everybody in the network.  I checked a sampling of Free Culture projects and got surprisingly good results already, but it probably couldn't hurt to ensure coverage.

## Stenotype Corner

It almost seems time to retire Stenotype Corner, as I write this lucky thirteenth week's installment. Again, I don't have anything of interest to report, except that I continue to use the keyboard to type routine text and gain confidence.

Due to the kind of typing that I needed to do this week, requiring speed and fussy words, my overall use tapered off, but a lot of this post comes to you via stenography...

## Entropy Arbitrage

{% github jcolag/entropy-arbitrage %}

Mostly, this week brings cosmetic changes. The link to the Markdown source for each post on GitHub no longer has an annotation icon. Code blocks look less sloppy. The table of contents slide-out tray now uses the bulleted list icon that it always should have, and the slide-outs also have a minimum width for better readability on small screens. Repository previews take a little extra space. Any links to MSNBC will now show with the NBC annotation icon, though I don't know if any have shown, yet.

More substantial, the tag generation script now does a better job at dealing with variations on a name.

## Notoboto

{% github jcolag/Notoboto %}

The lack of non-Latin characters turned out to get further under my skin than I expected, not because I *need* it to work so much as it felt seriously wrong.  As such, I snagged `data/data.json` from [Transliteration](https://github.com/yf-hk/transliteration), a JavaScript Unicode-to-Latin transliteration library.  I don't really care about the code, but the JSON file provides a compact presentation of the various writing systems. In fact, I chose this over the more famous transliteration projects, such as the many descendants of [Unidecode](https://metacpan.org/pod/Text::Unidecode) that you can find for almost any language, and which do the same thing, because it uses a more compact form of the data.

With that in hand, I overhauled the `slugFromTitle` function to drop the decomposition-and-strip-non-ASCII process, and replaced it with a character-by-character transliteration, since this table already does the same job invisibly.

```Tcl
set ascii [lmap c $chars {
  set high [expr { ($c >> 8) & 0xFF }]
  set low [expr { $c & 0xFF }]
  set row [dict get $transliteration $high]
  set xlit [lindex $row $low]
  ::unicode::fromstring $xlit
}]
```

I actually note this in case anybody wants to use the same data for a similar process, down the line, not because anybody needs to see this particular block of code.  You need the sixteen-bit Unicode code-point for each character---conveniently what Tcl's `::unicode::fromstring` supplies---and you can index the table using the high byte of the code-point to choose the row, and the low byte to choose the element of the row.  For consistency, the code then turns the resulting string into another list of code-points for further processing, which the function later flattens into a single dimension.

The code then still deletes all remaining non-ASCII characters, so that we don't need to worry about the pronunciation for emoji or mathematical symbols, and forms a flat string.  Then it handles the conversion to lowercase, the removal of stop-words, and so forth, instead of doing that at the start.

As a result, if I had such a horrifying note, I can now feed in a title that looks like this mess.

> 早上好/ 早 ! المعذرة / 어디예요 : Здравствуйте ~ áéíóúÁÉÍÓÚ ñçÑÇ üÜ öÖ

And it returns something that looks like this.

> zao-shang-hao-zao-lmaadhr-eodiyeyo-zdravstvuyte-aeiouaeiou-ncnc-uu-oo

Add the [ULID](https://github.com/ulid/spec) from last week, and I think that we have a winner.  It doesn't work *perfectly*:  It flattens a lot of nuance in languages with complex pronunciation rules, because it only works character-by-character.  And it especially goes awry in cases where multiple languages use the same script for different purpose, such Japanese adapting Chinese ideograms, where they use different pronunciations, since our table can only choose one possibility app and can't guess languages.  But ignoring those details, this creates a roughly pronounceable and---more importantly, to me---recognizable blob for any script. And so, I can now find notes without searching the files or even opening **Notoboto** at all, if I need something in a hurry.

## Next

I have some small changes to make on the blog, and some clean-up to do on **Notoboto**. After that, though, I don't actually have a plan, since a lot of my time will probably go to settling into the new laptop.

* * *

**Credits**:  The header image is [A View of the Plundering and Burning of the City of Grymross](https://www.gallery.ca/collection/artwork/a-view-of-the-plundering-and-burning-of-the-city-of-grimross) by Thomas Davies, long in the public domain due to copyright expiration.
